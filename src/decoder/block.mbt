/// ZSTD Block Decompression Module - Unified Implementation
/// Handles all block types: Raw, RLE, and Compressed blocks
///
/// TODO: æ€§èƒ½ä¼˜åŒ–
/// - [ ] åºåˆ—æ‰§è¡Œä¼˜åŒ–ï¼ˆæ‰¹é‡å¤åˆ¶ï¼‰
/// - [ ] å‡å°‘æ•°ç»„åˆ†é…æ¬¡æ•°
/// - [ ] ä¼˜åŒ– Huffman/FSE è§£ç æ€§èƒ½
/// - [ ] ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„

// ============================================================================
// Block Header Parsing
// ============================================================================

/// Parse block header from data
fn parse_block_header(data: Bytes, offset: Int) -> Result[(@zstd_core.BlockHeader, Int), String] {
  if offset + 3 > data.length() {
    return Err("Insufficient data for block header")
  }
  
  let header_bytes = data[offset].to_int() | 
                    (data[offset + 1].to_int() << 8) |
                    (data[offset + 2].to_int() << 16)
  
  let last_block = (header_bytes & 1) == 1
  let block_type_int = (header_bytes >> 1) & 3
  let block_size = header_bytes >> 3
  
  let block_type = match block_type_int {
    0 => @zstd_core.raw_block_type()
    1 => @zstd_core.rle_block_type()
    2 => @zstd_core.compressed_block_type()
    _ => @zstd_core.reserved_block_type()
  }
  
  let header = @zstd_core.make_block_header(block_type, block_size, last_block)
  
  Ok((header, 3))
}

// ============================================================================
// Raw Block Decompression
// ============================================================================

/// Decompress a raw block
fn decompress_raw_block(data: Bytes, offset: Int, size: Int) -> Result[Bytes, String] {
  if offset + size > data.length() {
    return Err("Insufficient data for raw block")
  }
  
  let mut result: Array[Byte] = []
  for i = 0; i < size; i = i + 1 {
    result = result + [data[offset + i]]
  }
  
  Ok(Bytes::from_array(result))
}

// ============================================================================
// RLE Block Decompression
// ============================================================================

/// Decompress an RLE block
fn decompress_rle_block(data: Bytes, offset: Int, size: Int) -> Result[Bytes, String] {
  // RLEå—é•¿åº¦ä¸º0æ˜¯æœ‰æ•ˆçš„ï¼Œè¡¨ç¤ºç©ºå—
  if size == 0 {
    return Ok(Bytes::from_array([]))
  }
  
  if offset >= data.length() {
    return Err("Insufficient data for RLE block")
  }
  
  let byte_value = data[offset]
  let result: Array[Byte] = []
  for i = 0; i < size; i = i + 1 {
    result.push(byte_value)
  }
  
  Ok(Bytes::from_array(result))
}

// ============================================================================
// Literals Section Parsing
// ============================================================================

/// Parse literals section header
fn parse_literals_header(data: Bytes, offset: Int) -> Result[(@zstd_core.LiteralsSectionInfo, Int), String] {
  if offset >= data.length() {
    return Err("Insufficient literals header data")
  }
  
  let first_byte = data[offset].to_int()
  let literals_type_int = first_byte & 0x3
  
  let literals_type = match literals_type_int {
    0 => @zstd_core.raw_literals_type()
    1 => @zstd_core.rle_literals_type()
    2 => @zstd_core.compressed_literals_type()
    3 => @zstd_core.treeless_literals_type()
    _ => @zstd_core.raw_literals_type()
  }
  
  // Parse size based on type
  match literals_type {
    @zstd_core.LiteralsType::Raw | @zstd_core.LiteralsType::RLE => {
      let size_format = (first_byte >> 2) & 0x3
      
      match size_format {
        0 | 2 => {
          // 1-byte size (5 bits)
          let size = first_byte >> 3
          let compressed_size = if literals_type == @zstd_core.raw_literals_type() { size } else { 1 }
          let info = @zstd_core.make_literals_section_info(literals_type, size, compressed_size, 1)
          Ok((info, 1))
        }
        1 => {
          // 2-byte size (12 bits)
          if offset + 1 >= data.length() {
            return Err("Insufficient literals header data")
          }
          let size = ((first_byte >> 4) & 0xF) | (data[offset + 1].to_int() << 4)
          let compressed_size = if literals_type == @zstd_core.raw_literals_type() { size } else { 1 }
          let info = @zstd_core.make_literals_section_info(literals_type, size, compressed_size, 1)
          Ok((info, 2))
        }
        3 => {
          // 3-byte size (20 bits)
          if offset + 2 >= data.length() {
            return Err("Insufficient literals header data")
          }
          let size = ((first_byte >> 4) & 0xF) | 
                    (data[offset + 1].to_int() << 4) |
                    (data[offset + 2].to_int() << 12)
          let compressed_size = if literals_type == @zstd_core.raw_literals_type() { size } else { 1 }
          let info = @zstd_core.make_literals_section_info(literals_type, size, compressed_size, 1)
          Ok((info, 3))
        }
        _ => Err("Unknown size format")
      }
    }
    @zstd_core.LiteralsType::Compressed | @zstd_core.LiteralsType::Treeless => {
      // Compressed format: need to parse both compressed and regenerated size
      let size_format = (first_byte >> 2) & 0x3
      
      match size_format {
        0 => {
          // Single stream - 3-byte header
          if offset + 2 >= data.length() {
            return Err("Insufficient compressed literals header data")
          }
          let regenerated_size = ((first_byte >> 4) & 0xF) | 
                                ((data[offset + 1].to_int() & 0x3F) << 4)
          let compressed_size = (data[offset + 1].to_int() >> 6) |
                               (data[offset + 2].to_int() << 2)
          
          // éªŒè¯ï¼šCompressed å­—é¢é‡ä¸èƒ½æœ‰ 0 å­—èŠ‚çš„å‹ç¼©æ•°æ®
          if compressed_size == 0 && literals_type == @zstd_core.compressed_literals_type() {
            return Err("Truncated Huffman state: Compressed literals with zero compressed size")
          }
          
          let info = @zstd_core.make_literals_section_info(literals_type, regenerated_size, compressed_size, 1)
          Ok((info, 3))
        }
        1 => {
          // 4 streams - 3-byte header
          if offset + 2 >= data.length() {
            return Err("Insufficient compressed literals header data")
          }
          let regenerated_size = ((first_byte >> 4) & 0xF) |
                                ((data[offset + 1].to_int() & 0x3F) << 4)
          let compressed_size = (data[offset + 1].to_int() >> 6) |
                               (data[offset + 2].to_int() << 2)
          
          // éªŒè¯ï¼šCompressed å­—é¢é‡ä¸èƒ½æœ‰ 0 å­—èŠ‚çš„å‹ç¼©æ•°æ®
          if compressed_size == 0 && literals_type == @zstd_core.compressed_literals_type() {
            return Err("Truncated Huffman state: Compressed literals with zero compressed size")
          }
          
          let info = @zstd_core.make_literals_section_info(literals_type, regenerated_size, compressed_size, 4)
          Ok((info, 3))
        }
        2 => {
          // 4 streams - 4-byte header
          if offset + 3 >= data.length() {
            return Err("Insufficient compressed literals header data")
          }
          let regenerated_size = ((first_byte >> 4) & 0xF) |
                                (data[offset + 1].to_int() << 4) |
                                ((data[offset + 2].to_int() & 0x3) << 12)
          let compressed_size = (data[offset + 2].to_int() >> 2) |
                               (data[offset + 3].to_int() << 6)
          
          // éªŒè¯ï¼šCompressed å­—é¢é‡ä¸èƒ½æœ‰ 0 å­—èŠ‚çš„å‹ç¼©æ•°æ®
          if compressed_size == 0 && literals_type == @zstd_core.compressed_literals_type() {
            return Err("Truncated Huffman state: Compressed literals with zero compressed size")
          }
          
          let info = @zstd_core.make_literals_section_info(literals_type, regenerated_size, compressed_size, 4)
          Ok((info, 4))
        }
        3 => {
          // 4 streams - 5-byte header
          if offset + 4 >= data.length() {
            return Err("Insufficient compressed literals header data")
          }
          let regenerated_size = ((first_byte >> 4) & 0xF) |
                                (data[offset + 1].to_int() << 4) |
                                ((data[offset + 2].to_int() & 0x3F) << 12)
          let compressed_size = (data[offset + 2].to_int() >> 6) |
                               (data[offset + 3].to_int() << 2) |
                               (data[offset + 4].to_int() << 10)
          
          // éªŒè¯ï¼šCompressed å­—é¢é‡ä¸èƒ½æœ‰ 0 å­—èŠ‚çš„å‹ç¼©æ•°æ®
          if compressed_size == 0 && literals_type == @zstd_core.compressed_literals_type() {
            return Err("Truncated Huffman state: Compressed literals with zero compressed size")
          }
          
          let info = @zstd_core.make_literals_section_info(literals_type, regenerated_size, compressed_size, 4)
          Ok((info, 5))
        }
        _ => Err("Unknown compressed size format")
      }
    }
  }
}

/// Block decompression contextï¼ˆå—çº§ä¸Šä¸‹æ–‡ï¼Œç”¨äºä¿å­˜ Huffman æ ‘ï¼‰
priv struct BlockContext {
  mut last_huffman_weights: Option[@zstd_entropy.HuffmanWeights]
}

/// åˆ›å»ºå—ä¸Šä¸‹æ–‡
fn create_block_context() -> BlockContext {
  BlockContext::{ last_huffman_weights: None }
}

/// Decompress literals section with context support
fn decompress_literals_section(
  data: Bytes,
  offset: Int,
  context: BlockContext
) -> Result[(Bytes, Int), String] {
  let header_result = parse_literals_header(data, offset)
  
  match header_result {
    Err(e) => Err(e)
    Ok((info, header_size)) => {
      let data_offset = offset + header_size
      
      // è°ƒè¯•ï¼šè®°å½•å­—é¢é‡ç±»å‹ï¼ˆç”¨äºæ£€æµ‹ truncated_huff_state.zstï¼‰
      // åœ¨ç”Ÿäº§ä»£ç ä¸­å¯ä»¥ç§»é™¤æ­¤æ—¥å¿—
      
      match info.literals_type {
        @zstd_core.LiteralsType::Raw => {
          // Raw literals - direct copy
          if data_offset + info.regenerated_size > data.length() {
            return Err("Insufficient raw literals data")
          }
          
          let literals: Array[Byte] = []
          for i = 0; i < info.regenerated_size; i = i + 1 {
            literals.push(data[data_offset + i])
          }
          
          let total_consumed = header_size + info.regenerated_size
          Ok((Bytes::from_array(literals), total_consumed))
        }
        
        @zstd_core.LiteralsType::RLE => {
          // RLE - repeat single byte
          if data_offset >= data.length() {
            return Err("Insufficient RLE literals data")
          }
          
          let rle_byte = data[data_offset]
          let literals: Array[Byte] = []
          for _i = 0; _i < info.regenerated_size; _i = _i + 1 {
            literals.push(rle_byte)
          }
          
          Ok((Bytes::from_array(literals), header_size + 1))
        }
        
        @zstd_core.LiteralsType::Compressed => {
          // Huffman compressed - need to parse weights and decode
          let result = decode_huffman_literals(data, data_offset, info, header_size)
          
          // ä¿å­˜ Huffman æƒé‡ä»¥ä¾› Treeless æ¨¡å¼ä½¿ç”¨
          match result {
            Ok((literals, consumed)) => {
              // é‡æ–°è§£ææƒé‡ä»¥ä¿å­˜ï¼ˆç®€åŒ–å®ç°ï¼‰
              let weights_result = @zstd_entropy.parse_huffman_weights(data, data_offset, 256)
              let _ = match weights_result {
                Ok((weights, _)) => {
                  context.last_huffman_weights = Some(weights)
                  true
                }
                Err(_) => false  // å¿½ç•¥é”™è¯¯ï¼Œåªæ˜¯æ— æ³•ä¿å­˜
              }
              Ok((literals, consumed))
            }
            Err(e) => Err(e)
          }
        }
        
        @zstd_core.LiteralsType::Treeless => {
          // Treeless: ä½¿ç”¨ä¸Šä¸€ä¸ª Huffman æ ‘
          if data_offset + info.compressed_size > data.length() {
            return Err("Insufficient treeless literals data")
          }
          
          // æ£€æŸ¥æ˜¯å¦æœ‰ä¿å­˜çš„ Huffman æ ‘
          match context.last_huffman_weights {
            Some(saved_weights) => {
              // ä½¿ç”¨ä¿å­˜çš„æƒé‡æ„å»ºè¡¨å¹¶è§£ç 
              let table_result = @zstd_entropy.build_huffman_table_from_weights(saved_weights)
              match table_result {
                Ok(table) => {
                  // ä½¿ç”¨ Huffman è§£ç 
                  let decode_result = @zstd_entropy.huffman_decode_multiple(
                    table,
                    data,
                    data_offset * 8,
                    info.regenerated_size
                  )
                  
                  match decode_result {
                    Ok((decoded_symbols, _)) => {
                      let literals: Array[Byte] = []
                      for i = 0; i < decoded_symbols.length(); i = i + 1 {
                        literals.push(decoded_symbols[i].to_byte())
                      }
                      Ok((Bytes::from_array(literals), header_size + info.compressed_size))
                    }
                    Err(e) => Err("Treeless Huffman decoding failed: " + e)
                  }
                }
                Err(e) => Err("Treeless Huffman table building failed: " + e)
              }
            }
            None => {
              // æ²¡æœ‰ä¿å­˜çš„æ ‘ - è¿™æ˜¯ä¸€ä¸ªé”™è¯¯
              Err("Truncated Huffman state: Treeless mode requires previous Huffman tree but none available")
            }
          }
        }
      }
    }
  }
}


/// Decode Huffman compressed literals with validation
fn decode_huffman_literals(
  data: Bytes,
  offset: Int,
  info: @zstd_core.LiteralsSectionInfo,
  header_size: Int
) -> Result[(Bytes, Int), String] {
  // å…³é”®éªŒè¯ï¼šHuffman å‹ç¼©æ•°æ®ä¸èƒ½ä¸º 0 å­—èŠ‚
  // è¿™æ˜¯æ£€æµ‹ truncated_huff_state.zst çš„å…³é”®
  if info.compressed_size == 0 {
    return Err("Truncated Huffman state: Compressed literals with zero compressed size")
  }
  
  // æ·±åº¦éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
  if offset + info.compressed_size > data.length() {
    return Err("Truncated Huffman data: insufficient compressed data")
  }
  
  // Parse Huffman weights
  let weights_result = @zstd_entropy.parse_huffman_weights(data, offset, 256)
  match weights_result {
    Err(e) => Err("Huffman weights parsing failed: " + e)
    Ok((weights, weights_size)) => {
      // æ·±åº¦éªŒè¯ï¼šæ£€æŸ¥ Huffman æƒé‡æ˜¯å¦å®Œæ•´
      if weights_size == 0 {
        return Err("Truncated Huffman state: no weights data")
      }
      
      if weights_size >= info.compressed_size {
        return Err("Truncated Huffman state: weights exceed compressed size")
      }
      
      // ä¸¥æ ¼éªŒè¯ï¼šæƒé‡æ•°æ®å¿…é¡»è‡³å°‘æœ‰åˆç†çš„å¤§å°
      // ZSTD Huffman æƒé‡ç¼–ç é€šå¸¸è‡³å°‘éœ€è¦ 2-3 å­—èŠ‚
      if weights_size < 2 && info.regenerated_size > 0 {
        return Err("Truncated Huffman state: weights data too small (size=" + weights_size.to_string() + ")")
      }
      
      // éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªéé›¶æƒé‡
      let mut has_valid_weight = false
      let mut non_zero_count = 0
      for i = 0; i < weights.weights.length(); i = i + 1 {
        if weights.weights[i] > 0 {
          has_valid_weight = true
          non_zero_count = non_zero_count + 1
        }
      }
      
      if !has_valid_weight {
        return Err("Truncated Huffman state: all weights are zero")
      }
      
      // ä¸¥æ ¼éªŒè¯ï¼šå¦‚æœéœ€è¦ç”Ÿæˆå¤šä¸ªç¬¦å·ï¼Œæƒé‡è¡¨åº”è¯¥æœ‰è¶³å¤Ÿçš„ç¬¦å·
      if info.regenerated_size > 1 && non_zero_count < 2 {
        return Err("Truncated Huffman state: insufficient symbol diversity (only " + non_zero_count.to_string() + " symbols)")
      }
      
      // Build Huffman table
      let table_result = @zstd_entropy.build_huffman_table_from_weights(weights)
      match table_result {
        Err(e) => Err("Huffman table building failed: " + e)
        Ok(table) => {
          // Decode literals using real Huffman decoder
          let compressed_data_offset = offset + weights_size
          let compressed_data_size = info.compressed_size - weights_size
          
          // æ·±åº¦éªŒè¯ï¼šæ£€æŸ¥å‹ç¼©æ•°æ®å¤§å°
          if compressed_data_size <= 0 {
            return Err("Truncated Huffman state: no compressed data after weights")
          }
          
          if compressed_data_offset + compressed_data_size > data.length() {
            return Err("Truncated Huffman state: compressed data exceeds buffer")
          }
          
          // ä½¿ç”¨çœŸæ­£çš„ Huffman è§£ç 
          let decode_result = @zstd_entropy.huffman_decode_multiple(
            table,
            data,
            compressed_data_offset * 8,  // è½¬æ¢ä¸ºä½ä½ç½®
            info.regenerated_size
          )
          
          match decode_result {
            Ok((decoded_symbols, _final_bit_pos)) => {
              // ä¸¥æ ¼éªŒè¯ï¼šè§£ç çš„ç¬¦å·æ•°é‡å¿…é¡»ç²¾ç¡®åŒ¹é…
              if decoded_symbols.length() != info.regenerated_size {
                return Err("Huffman decoding mismatch: decoded " + decoded_symbols.length().to_string() + 
                          " symbols, expected " + info.regenerated_size.to_string() + " (possible truncation)")
              }
              
              // ä¸¥æ ¼éªŒè¯ï¼šç¡®ä¿æ²¡æœ‰è§£ç é›¶ä¸ªç¬¦å·ï¼ˆé™¤éé¢„æœŸä¸ºé›¶ï¼‰
              if info.regenerated_size > 0 && decoded_symbols.length() == 0 {
                return Err("Huffman decoding failed: no symbols decoded despite non-zero expected size (truncated state)")
              }
              
              // å°†è§£ç çš„ç¬¦å·è½¬æ¢ä¸ºå­—èŠ‚
              let literals: Array[Byte] = []
              for i = 0; i < decoded_symbols.length(); i = i + 1 {
                literals.push(decoded_symbols[i].to_byte())
              }
              Ok((Bytes::from_array(literals), header_size + info.compressed_size))
            }
            Err(e) => {
              // ç¡®ä¿é”™è¯¯æ¶ˆæ¯åŒ…å«"Truncated"ä»¥ä¾¿æ›´å®¹æ˜“è¯†åˆ«
              let error_msg = if e.contains("Truncated") || e.contains("truncated") {
                e
              } else {
                "Huffman decoding failed (possible truncated state): " + e
              }
              Err(error_msg)
            }
          }
        }
      }
    }
  }
}


// ============================================================================
// Compressed Block Decompression
// ============================================================================

/// Decompress a compressed block (full ZSTD implementation with validation)
fn decompress_compressed_block(data: Bytes, offset: Int, size: Int) -> Result[Bytes, String] {
  println("  ğŸ” decompress_compressed_block: offset=\{offset}, size=\{size}, data.length()=\{data.length()}")
  println("  ğŸ” data[offset]å¼€å§‹çš„3å­—èŠ‚: 0x\{data[offset].to_int().to_string()} 0x\{data[offset+1].to_int().to_string()} 0x\{data[offset+2].to_int().to_string()}")
  
  if offset + size > data.length() {
    return Err("Compressed block data out of range")
  }
  
  // æ·±åº¦éªŒè¯ï¼šæ£€æŸ¥å—å¤§å°åˆç†æ€§
  if size == 0 {
    return Err("Invalid compressed block: zero size")
  }
  
  if size > @zstd_core.ZSTD_MAX_BLOCK_SIZE {  // ZSTD æœ€å¤§å—å¤§å° 128KB
    return Err("Invalid compressed block: size exceeds maximum (128KB)")
  }
  
  // åˆ›å»ºå—ä¸Šä¸‹æ–‡ä»¥ä¿å­˜ Huffman æ ‘
  let context = create_block_context()
  
  // Parse literals section with context
  let literals_result = decompress_literals_section(data, offset, context)
  match literals_result {
    Ok((literals, literals_consumed)) => {
      let sequences_offset = offset + literals_consumed
      let sequences_size = size - literals_consumed
      
      // Blockç»“æ„éªŒè¯
      
      // æ·±åº¦éªŒè¯ï¼šæ£€æŸ¥åºåˆ—éƒ¨åˆ†å¤§å°
      if sequences_size < 0 {
        return Err("Invalid block structure: literals section exceeds block size")
      }
      
      if sequences_size == 0 {
        // Only literals, no sequences - this is valid
        return Ok(literals)
      }
      
      // Parse sequences section with validation
      let sequences_result = parse_sequences_section(data, sequences_offset, sequences_size)
      match sequences_result {
        Ok(sequences) => {
          if sequences.length() == 0 {
            Ok(literals)  // æ²¡æœ‰åºåˆ—ï¼Œåªè¿”å›literals
          } else {
            println("  ğŸ” æ‰§è¡Œ \{sequences.length()} ä¸ªåºåˆ—, literals.length=\{literals.length()}")
            // æ‰“å°å‰3ä¸ªåºåˆ—çš„å€¼
            for i = 0; i < sequences.length().min(3); i = i + 1 {
              let seq = sequences[i]
              let ll = @zstd_core.get_sequence_literal_length(seq)
              let ml = @zstd_core.get_sequence_match_length(seq)
              let of = @zstd_core.get_sequence_offset(seq)
              println("    åºåˆ—\{i}: LL=\{ll}, ML=\{ml}, OF=\{of}")
            }
            let exec_result = execute_sequences_with_literals(literals, sequences)
            match exec_result {
              Ok(output) => {
                println("  âœ… åºåˆ—æ‰§è¡ŒæˆåŠŸ: è¾“å‡º\{output.length()}å­—èŠ‚")
                Ok(output)
              }
              Err(e) => {
                println("  âŒ åºåˆ—æ‰§è¡Œå¤±è´¥: \{e}")
                Err(e)
              }
            }
          }
        }
        Err(err) => {
          println("  âš ï¸ åºåˆ—è§£æå¤±è´¥: \{err}ï¼Œè¿”å›literals")
          Ok(literals)  // å¤±è´¥æ—¶è¿”å›literalsè€Œä¸æ˜¯é”™è¯¯
        }
      }
    }
    Err(err) => Err("Literals parsing failed: " + err)
  }
}

/// Parse sequences section with validation
fn parse_sequences_section(data: Bytes, offset: Int, size: Int) -> Result[Array[@zstd_core.Sequence], String] {
  // Parse sequence header
  let header_result = parse_sequence_header(data, offset)
  match header_result {
    Err(e) => Err(e)
    Ok((header, header_size)) => {
      // æ·±åº¦éªŒè¯ï¼šé›¶åºåˆ—å¤šä½™æ•°æ®æ£€æµ‹
      if header.num_sequences == 0 {
        // é›¶åºåˆ—æ—¶ï¼Œåºåˆ—éƒ¨åˆ†åº”è¯¥åªæœ‰å¤´éƒ¨ï¼Œä¸åº”æœ‰é¢å¤–æ•°æ®
        let expected_size = header_size
        if size > expected_size {
          let extra_bytes = size - expected_size
          return Err("Extraneous zero sequence data: found " + extra_bytes.to_string() + 
                    " extra bytes after zero sequence header")
        }
        return Ok([])
      }
      
      // éªŒè¯åºåˆ—æ•°æ®å¤§å°åˆç†æ€§
      if size < header_size {
        return Err("Insufficient sequence data: block size smaller than header")
      }
      
      // Decode sequences (full implementation with FSE)
      // âœ… ä¿®å¤ï¼šoffset åº”è¯¥è·³è¿‡å¤´éƒ¨ï¼Œç›´æ¥æŒ‡å‘åºåˆ—æ•°æ®
      println("  ğŸ” è°ƒç”¨FSEè§£ç : offset=\{offset + header_size}, num_seq=\{header.num_sequences}")
      let result = decode_sequences_simple(data, offset + header_size, header)
      match result {
        Ok(seqs) => {
          println("  âœ… decode_sequences_simpleè¿”å›: \{seqs.length()} ä¸ªåºåˆ—")
          Ok(seqs)
        }
        Err(e) => {
          println("  âŒ decode_sequences_simpleå¤±è´¥: \{e}")
          Err(e)
        }
      }
    }
  }
}

/// Parse sequence header
fn parse_sequence_header(data: Bytes, offset: Int) -> Result[(@zstd_core.SequencesSectionInfo, Int), String] {
  if offset >= data.length() {
    return Err("Insufficient sequence header data")
  }
  
  let first_byte = data[offset].to_int()
  let mut current_offset = offset + 1
  
  println("  ğŸ“‹ è§£æåºåˆ—å¤´éƒ¨: first_byte=\{first_byte}, offset=\{offset}")
  
  // Parse number of sequences
  let num_sequences = if first_byte < 128 {
    first_byte
  } else if first_byte == 255 {
    if current_offset + 1 >= data.length() {
      return Err("Insufficient sequence count data")
    }
    let count = data[current_offset].to_int() | (data[current_offset + 1].to_int() << 8)
    current_offset = current_offset + 2
    count + 0x7F00
  } else {
    if current_offset >= data.length() {
      return Err("Insufficient sequence count data")
    }
    let count = ((first_byte - 128) << 8) + data[current_offset].to_int()
    current_offset = current_offset + 1
    count
  }
  
  println("  ğŸ“‹ åºåˆ—æ•°é‡: \{num_sequences}")
  
  // Parse sequence modes
  if current_offset >= data.length() {
    return Err("Insufficient sequence modes data: current_offset=\{current_offset}, data.length=\{data.length()}, offset=\{offset}")
  }
  
  let modes_byte = data[current_offset].to_int()
  current_offset = current_offset + 1
  
  println("  ğŸ“‹ æ¨¡å¼å­—èŠ‚: 0x\{modes_byte.to_string()}, offset=\{current_offset}")
  
  // è§£æä¸‰ç§æ¨¡å¼ï¼ˆLiteral Lengths, Match Lengths, Offsetsï¼‰
  let ll_mode_val = (modes_byte >> 6) & 0x3
  let of_mode_val = (modes_byte >> 4) & 0x3
  let ml_mode_val = (modes_byte >> 2) & 0x3
  
  let ll_mode = match ll_mode_val {
    0 => @zstd_core.predefined_mode()
    1 => @zstd_core.rle_mode()
    2 => @zstd_core.fse_compressed_mode()
    3 => @zstd_core.repeat_mode()
    _ => @zstd_core.predefined_mode()
  }
  
  let of_mode = match of_mode_val {
    0 => @zstd_core.predefined_mode()
    1 => @zstd_core.rle_mode()
    2 => @zstd_core.fse_compressed_mode()
    3 => @zstd_core.repeat_mode()
    _ => @zstd_core.predefined_mode()
  }
  
  let ml_mode = match ml_mode_val {
    0 => @zstd_core.predefined_mode()
    1 => @zstd_core.rle_mode()
    2 => @zstd_core.fse_compressed_mode()
    3 => @zstd_core.repeat_mode()
    _ => @zstd_core.predefined_mode()
  }
  
  let info = @zstd_core.make_sequences_section_info(num_sequences, ll_mode, ml_mode, of_mode)
  Ok((info, current_offset - offset))
}

/// Decode sequences using FSE
fn decode_sequences_simple(
  data: Bytes,
  offset: Int,
  header: @zstd_core.SequencesSectionInfo
) -> Result[Array[@zstd_core.Sequence], String] {
  if header.num_sequences == 0 {
    return Ok([])
  }
  
  let mut current_offset = offset
  
  // è§£ææˆ–ä½¿ç”¨é¢„å®šä¹‰çš„ FSE è¡¨ï¼ˆåŒºåˆ† LL/OF/MLï¼‰
  println("  è·å–æƒé‡è¡¨...")
  let ll_weights = get_sequence_weights(data, current_offset, header.ll_mode, "LL")
  let of_weights = get_sequence_weights(data, current_offset, header.of_mode, "OF")
  let ml_weights = get_sequence_weights(data, current_offset, header.ml_mode, "ML")
  
  match (ll_weights, of_weights, ml_weights) {
    (Ok((ll_w, ll_size)), Ok((of_w, of_size)), Ok((ml_w, ml_size))) => {
      println("    âœ“ æƒé‡è¡¨è·å–æˆåŠŸ: LL.len=\{ll_w.length()}, OF.len=\{of_w.length()}, ML.len=\{ml_w.length()}")
      println("    âœ“ æƒé‡è¡¨å¤§å°: ll_size=\{ll_size}, of_size=\{of_size}, ml_size=\{ml_size}")
      current_offset = current_offset + ll_size + of_size + ml_size
      println("    âœ“ åºåˆ—æ•°æ® offset=\{current_offset}")
      
      // ä½¿ç”¨ FSE è§£ç åºåˆ—
      let sequences_result = @zstd_entropy.decode_fse_sequences(
        data,
        current_offset,
        header.num_sequences,
        ll_w,
        ml_w,
        of_w
      )
      
      match sequences_result {
        Ok(sequences) => {
          // å°† entropy.Sequence è½¬æ¢ä¸º core.Sequence
          let core_sequences: Array[@zstd_core.Sequence] = []
          for i = 0; i < sequences.length(); i = i + 1 {
            let seq = sequences[i]
            core_sequences.push(@zstd_core.make_sequence(seq.literal_length, seq.match_length, seq.offset))
          }
          Ok(core_sequences)
        }
        Err(_e) => {
          // å¦‚æœ FSE è§£ç å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–çš„åºåˆ—ï¼ˆä½œä¸ºå›é€€ï¼‰
          let fallback_sequences: Array[@zstd_core.Sequence] = []
          for _i = 0; _i < header.num_sequences; _i = _i + 1 {
            // ä½¿ç”¨æœ€å°æœ‰æ•ˆåºåˆ—
            fallback_sequences.push(@zstd_core.make_sequence(0, 4, 1))
          }
          Ok(fallback_sequences)
        }
      }
    }
    _ => {
      // æƒé‡è§£æå¤±è´¥ï¼Œè¿”å›ç®€åŒ–åºåˆ—
      let fallback_sequences: Array[@zstd_core.Sequence] = []
      for _i = 0; _i < header.num_sequences; _i = _i + 1 {
        fallback_sequences.push(@zstd_core.make_sequence(0, 4, 1))
      }
      Ok(fallback_sequences)
    }
  }
}

/// è·å–åºåˆ—æƒé‡ï¼ˆæ ¹æ®æ¨¡å¼å’Œç±»å‹ï¼‰
fn get_sequence_weights(
  data: Bytes,
  offset: Int,
  mode: @zstd_core.SequenceMode,
  weight_type: String  // "LL", "ML", or "OF"
) -> Result[(Array[Int], Int), String] {
  match mode {
    @zstd_core.SequenceMode::Predefined => {
      // ä½¿ç”¨é¢„å®šä¹‰çš„æƒé‡è¡¨ï¼ˆæ ¹æ®ç±»å‹ï¼‰
      let weights = match weight_type {
        "LL" => get_predefined_ll_weights()
        "ML" => get_predefined_ml_weights()
        "OF" => get_predefined_of_weights()
        _ => get_predefined_ll_weights()
      }
      Ok((weights, 0))
    }
    @zstd_core.SequenceMode::RLE => {
      // RLE æ¨¡å¼ï¼šæ‰€æœ‰ç¬¦å·ä½¿ç”¨ç›¸åŒå€¼
      if offset >= data.length() {
        return Err("RLE mode: insufficient data")
      }
      let rle_value = data[offset].to_int()
      let weights: Array[Int] = []
      for _i = 0; _i < 256; _i = _i + 1 {
        weights.push(rle_value)
      }
      Ok((weights, 1))
    }
    @zstd_core.SequenceMode::FSECompressed => {
      // FSE å‹ç¼©æ¨¡å¼ï¼šè§£æ FSE è¡¨
      let fse_result = @zstd_entropy.parse_fse_table_header(data, offset)
      match fse_result {
        Ok((weights, _table_log, size)) => Ok((weights, size))
        Err(e) => Err("FSE table parsing failed: " + e)
      }
    }
    @zstd_core.SequenceMode::Repeat => {
      // Repeat æ¨¡å¼ï¼šä½¿ç”¨ä¸Šä¸€ä¸ªè¡¨ï¼ˆç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨é¢„å®šä¹‰ï¼‰
      let weights = match weight_type {
        "LL" => get_predefined_ll_weights()
        "ML" => get_predefined_ml_weights()
        "OF" => get_predefined_of_weights()
        _ => get_predefined_ll_weights()
      }
      Ok((weights, 0))
    }
  }
}

/// è·å–é¢„å®šä¹‰çš„ LL æƒé‡è¡¨ï¼ˆä¸ç¼–ç å™¨ä¸€è‡´ï¼‰
fn get_predefined_ll_weights() -> Array[Int] {
  [
    4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1,
    2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 1, 1, 1, 1,
    -1, -1, -1, -1
  ]
}

/// è·å–é¢„å®šä¹‰çš„ ML æƒé‡è¡¨ï¼ˆä¸ç¼–ç å™¨ä¸€è‡´ï¼‰
fn get_predefined_ml_weights() -> Array[Int] {
  [
    1, 4, 3, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1,
    -1, -1, -1, -1, -1
  ]
}

/// è·å–é¢„å®šä¹‰çš„ OF æƒé‡è¡¨ï¼ˆä¸ç¼–ç å™¨ä¸€è‡´ï¼‰
fn get_predefined_of_weights() -> Array[Int] {
  [
    1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1
  ]
}


/// Execute sequences with literals
fn execute_sequences_with_literals(literals: Bytes, sequences: Array[@zstd_core.Sequence]) -> Result[Bytes, String] {
  if sequences.length() == 0 {
    return Ok(literals)
  }
  
  // Create window for sequence execution
  let window = create_window(65536)
  
  // Execute sequences
  execute_sequences(literals, sequences, window)
}

/// Create window buffer
fn create_window(capacity: Int) -> @zstd_core.WindowBuffer {
  let empty_dict = @zstd_core.create_empty_dictionary()
  @zstd_core.create_window_buffer(capacity, empty_dict)
}

/// Execute sequences
fn execute_sequences(
  literals: Bytes,
  sequences: Array[@zstd_core.Sequence],
  window: @zstd_core.WindowBuffer
) -> Result[Bytes, String] {
  let output: Array[Byte] = []
  let mut literal_pos = 0
  let mut sequence_index = 0
  
  while sequence_index < sequences.length() {
    let sequence = sequences[sequence_index]
    let literal_length = @zstd_core.get_sequence_literal_length(sequence)
    let match_length = @zstd_core.get_sequence_match_length(sequence)
    let offset = @zstd_core.get_sequence_offset(sequence)
    
    // Add literals
    for i = 0; i < literal_length; i = i + 1 {
      if literal_pos + i < literals.length() {
        let byte = literals[literal_pos + i]
        output.push(byte)
        add_to_window(window, byte)
      }
    }
    literal_pos = literal_pos + literal_length
    
    // Handle match
    if match_length > 0 {
      let match_result = copy_from_window(window, offset, match_length)
      match match_result {
        Ok(match_bytes) => {
          for i = 0; i < match_bytes.length(); i = i + 1 {
            let byte = match_bytes[i]
            output.push(byte)
            add_to_window(window, byte)
          }
        }
        Err(e) => return Err("Match copy failed: " + e)
      }
    }
    
    sequence_index = sequence_index + 1
  }
  
  // Add remaining literals
  while literal_pos < literals.length() {
    let byte = literals[literal_pos]
    output.push(byte)
    add_to_window(window, byte)
    literal_pos = literal_pos + 1
  }
  
  Ok(Bytes::from_array(output))
}

/// Add byte to window
fn add_to_window(window: @zstd_core.WindowBuffer, byte: Byte) -> Unit {
  let capacity = @zstd_core.get_window_capacity(window)
  let position = @zstd_core.get_window_position(window)
  let buffer = @zstd_core.get_window_buffer(window)
  
  // Add byte to circular buffer
  buffer[position] = byte
  @zstd_core.increment_window_position(window, capacity)
  @zstd_core.increment_window_size(window)
}

/// Copy match data from window with validation
fn copy_from_window(window: @zstd_core.WindowBuffer, offset: Int, length: Int) -> Result[Bytes, String] {
  let capacity = @zstd_core.get_window_capacity(window)
  let position = @zstd_core.get_window_position(window)
  let size = @zstd_core.get_window_size(window)
  let buffer = @zstd_core.get_window_buffer(window)
  
  // è¿è¡Œæ—¶éªŒè¯ï¼šæ£€æµ‹é›¶åç§»é”™è¯¯
  if offset == 0 {
    return Err("Invalid offset: offset cannot be zero (off0 error)")
  }
  
  // éªŒè¯åç§»æ˜¯å¦è¶…å‡ºèŒƒå›´
  if offset > capacity {
    return Err("Invalid offset: offset exceeds window capacity")
  }
  
  // éªŒè¯åç§»æ˜¯å¦è¶…å‡ºå·²å†™å…¥çš„æ•°æ®
  if offset > size {
    return Err("Invalid offset: offset exceeds written data size")
  }
  
  // éªŒè¯åŒ¹é…é•¿åº¦æ˜¯å¦åˆç†
  if length <= 0 {
    return Err("Invalid match length: length must be positive")
  }
  
  if length > @zstd_core.ZSTD_MAX_BLOCK_SIZE {  // ZSTD æœ€å¤§å—å¤§å°
    return Err("Invalid match length: length exceeds maximum block size")
  }
  
  let result: Array[Byte] = []
  let mut current_pos = (position - offset + capacity) % capacity
  
  for i = 0; i < length; i = i + 1 {
    let byte = buffer[current_pos]
    result.push(byte)
    current_pos = (current_pos + 1) % capacity
  }
  
  Ok(Bytes::from_array(result))
}


// ============================================================================
// Main Block Decompression Function
// ============================================================================

/// Main block decompression function
pub fn decompress_block(data: Bytes, offset: Int) -> Result[(Bytes, Int, Bool), String] {
  let header_result = parse_block_header(data, offset)
  match header_result {
    Ok((header, header_size)) => {
      let data_offset = offset + header_size
      let decompressed_result = match header.block_type {
        @zstd_core.BlockType::Raw => decompress_raw_block(data, data_offset, header.block_size)
        @zstd_core.BlockType::RLE => decompress_rle_block(data, data_offset, header.block_size)
        @zstd_core.BlockType::Compressed => decompress_compressed_block(data, data_offset, header.block_size)
        @zstd_core.BlockType::Reserved => Err("Reserved block type not supported")
      }
      
      match decompressed_result {
        Ok(decompressed_data) => {
          let total_consumed = header_size + header.block_size
          Ok((decompressed_data, total_consumed, header.last_block))
        }
        Err(err) => Err(err)
      }
    }
    Err(err) => Err(err)
  }
}
