/// æµå¼æ“ä½œç¤ºä¾‹
/// å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ZSTD åº“è¿›è¡Œæµå¼å‹ç¼©å’Œè§£å‹ç¼©

/// æµå¼è§£å‹ç¼©ç¤ºä¾‹
pub fn streaming_decompression_example() -> Unit {
  println("=== æµå¼è§£å‹ç¼©ç¤ºä¾‹ ===")
  
  // åˆ›å»ºè§£å‹å™¨
  let decompressor = @api.create_decompressor()
  println("âœ… åˆ›å»ºè§£å‹å™¨æˆåŠŸ")
  
  // æ¨¡æ‹Ÿå¤šä¸ªæ•°æ®å—
  let chunks = [
    b"(\xb5/\xfd\x00\x00\x15\x00\x00\x00\x00",  // ç¬¬ä¸€ä¸ªå—
    b"More data here",                          // ç¬¬äºŒä¸ªå—
    b"Final chunk"                              // æœ€åä¸€ä¸ªå—
  ]
  
  let mut current_decompressor = decompressor
  let mut total_processed = 0
  
  // å¤„ç†æ¯ä¸ªæ•°æ®å—
  for i = 0; i < chunks.length(); i = i + 1 {
    let chunk = chunks[i]
    println("å¤„ç†å— " + (i + 1).to_string() + " (å¤§å°: " + chunk.length().to_string() + " å­—èŠ‚)")
    
    let (new_decompressor, result) = @api.decompress_with_decompressor(current_decompressor, chunk)
    current_decompressor = new_decompressor
    total_processed = total_processed + result.length()
    
    println("  å—å¤„ç†å®Œæˆï¼Œè¾“å‡ºå¤§å°: " + result.length().to_string() + " å­—èŠ‚")
  }
  
  println("æ€»è®¡å¤„ç†: " + total_processed.to_string() + " å­—èŠ‚")
}

/// æ¨¡æ‹Ÿå¤§æ–‡ä»¶å¤„ç†
pub fn large_file_processing_example() -> Unit {
  println("\n=== å¤§æ–‡ä»¶å¤„ç†ç¤ºä¾‹ ===")
  
  // æ¨¡æ‹Ÿä¸€ä¸ªå¤§æ–‡ä»¶çš„å¤šä¸ªå—
  let file_chunks = generate_sample_chunks(5)
  
  println("æ¨¡æ‹Ÿå¤„ç†å¤§æ–‡ä»¶ (" + file_chunks.length().to_string() + " ä¸ªå—)")
  
  let decompressor = @api.create_decompressor()
  let mut current_decompressor = decompressor
  let mut processed_chunks = 0
  let mut total_input_size = 0
  let mut total_output_size = 0
  
  for chunk in file_chunks {
    // åˆ†ææ¯ä¸ªå—
    let analysis = @api.analyze_zstd_file(chunk)
    
    if analysis.is_valid {
      println("âœ… å— " + (processed_chunks + 1).to_string() + ": " + 
              analysis.first_block_type + " å— (" + 
              chunk.length().to_string() + " å­—èŠ‚)")
      
      // è§£å‹ç¼©å—
      let (new_decompressor, result) = @api.decompress_with_decompressor(current_decompressor, chunk)
      current_decompressor = new_decompressor
      
      total_input_size = total_input_size + chunk.length()
      total_output_size = total_output_size + result.length()
    } else {
      println("âŒ å— " + (processed_chunks + 1).to_string() + ": æ— æ•ˆ ZSTD æ•°æ®")
    }
    
    processed_chunks = processed_chunks + 1
  }
  
  println("å¤„ç†å®Œæˆ:")
  println("  - å¤„ç†å—æ•°: " + processed_chunks.to_string())
  println("  - è¾“å…¥æ€»å¤§å°: " + total_input_size.to_string() + " å­—èŠ‚")
  println("  - è¾“å‡ºæ€»å¤§å°: " + total_output_size.to_string() + " å­—èŠ‚")
  
  if total_input_size > 0 {
    let ratio = @api.get_compression_ratio(total_output_size, total_input_size)
    println("  - å‹ç¼©ç‡: " + ratio.to_string() + "%")
  }
}

/// é”™è¯¯å¤„ç†ç¤ºä¾‹
pub fn error_handling_example() -> Unit {
  println("\n=== é”™è¯¯å¤„ç†ç¤ºä¾‹ ===")
  
  let test_cases = [
    ("æœ‰æ•ˆ ZSTD æ•°æ®", b"(\xb5/\xfd\x00\x00\x15\x00\x00\x00\x00"),
    ("æ— æ•ˆé­”æ•°", b"INVALID\x00\x00\x15\x00\x00\x00\x00"),
    ("æ•°æ®å¤ªçŸ­", b"(\xb5"),
    ("ç©ºæ•°æ®", b"")
  ]
  
  let decompressor = @api.create_decompressor()
  
  for case in test_cases {
    let (description, data) = case
    println("æµ‹è¯•: " + description)
    
    // éªŒè¯æ–‡ä»¶
    let (is_valid, message) = @api.validate_zstd_file(data)
    if is_valid {
      println("  âœ… éªŒè¯é€šè¿‡: " + message)
      
      // å°è¯•è§£å‹ç¼©
      let (_, result) = @api.decompress_with_decompressor(decompressor, data)
      println("  ğŸ“¤ è§£å‹ç¼©è¾“å‡º: " + result.length().to_string() + " å­—èŠ‚")
    } else {
      println("  âŒ éªŒè¯å¤±è´¥: " + message)
    }
  }
}

/// ç”Ÿæˆç¤ºä¾‹æ•°æ®å—
fn generate_sample_chunks(count: Int) -> Array[Bytes] {
  let mut chunks: Array[Bytes] = []
  
  for i = 0; i < count; i = i + 1 {
    // åˆ›å»ºä¸åŒç±»å‹çš„ç¤ºä¾‹å—
    let chunk = match i % 3 {
      0 => b"(\xb5/\xfd\x00\x00\x15\x00\x00\x00\x00"  // ç±»å‹ 0
      1 => b"(\xb5/\xfd\xa4\x00\x00\x10\x00\x02\x00"  // ç±»å‹ 1
      2 => b"(\xb5/\xfd\x00\x00\x85\x00\x00hHello"     // ç±»å‹ 2
      _ => b"(\xb5/\xfd\x00\x00\x15\x00\x00\x00\x00"  // é»˜è®¤
    }
    chunks = chunks + [chunk]
  }
  
  chunks
}

/// æ€§èƒ½ç›‘æ§ç¤ºä¾‹
pub fn performance_monitoring_example() -> Unit {
  println("\n=== æ€§èƒ½ç›‘æ§ç¤ºä¾‹ ===")
  
  let test_data = b"(\xb5/\xfd\x00\x00\x15\x00\x00\x00\x00"
  let iterations = 100
  
  println("æ‰§è¡Œ " + iterations.to_string() + " æ¬¡è§£å‹ç¼©æ“ä½œ...")
  
  let mut successful_operations = 0
  let mut total_input_bytes = 0
  let mut total_output_bytes = 0
  
  for i = 0; i < iterations; i = i + 1 {
    let analysis = @api.analyze_zstd_file(test_data)
    if analysis.is_valid {
      let result = @api.decompress(test_data)
      successful_operations = successful_operations + 1
      total_input_bytes = total_input_bytes + test_data.length()
      total_output_bytes = total_output_bytes + result.length()
    }
  }
  
  println("æ€§èƒ½ç»Ÿè®¡:")
  println("  - æˆåŠŸæ“ä½œ: " + successful_operations.to_string() + "/" + iterations.to_string())
  println("  - æ€»è¾“å…¥: " + total_input_bytes.to_string() + " å­—èŠ‚")
  println("  - æ€»è¾“å‡º: " + total_output_bytes.to_string() + " å­—èŠ‚")
  println("  - å¹³å‡è¾“å…¥å¤§å°: " + (total_input_bytes / successful_operations).to_string() + " å­—èŠ‚")
  println("  - å¹³å‡è¾“å‡ºå¤§å°: " + (total_output_bytes / successful_operations).to_string() + " å­—èŠ‚")
}

/// è¿è¡Œæ‰€æœ‰æµå¼ç¤ºä¾‹
pub fn run_streaming_examples() -> Unit {
  println("ğŸŒŠ MoonBit ZSTD æµå¼æ“ä½œç¤ºä¾‹")
  println("================================")
  
  streaming_decompression_example()
  large_file_processing_example()
  error_handling_example()
  performance_monitoring_example()
  
  println("\nâœ¨ æµå¼ç¤ºä¾‹æ¼”ç¤ºå®Œæˆï¼")
}